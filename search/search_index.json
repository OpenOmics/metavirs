{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"metavirs \ud83d\udd2c Viral Metagenomics Pipeline This is the home of the pipeline, metavirs. Its long-term goals: to assemble, annotate, and classify enviromental samples like no pipeline before! Overview \u00b6 Welcome to our documentation! This guide is the main source of documentation for users that are getting started with the viral metagenomics pipeline . The ./metavirs pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: metavirs run : Run the metavirs pipeline with your input files. metavirs unlock : Unlocks a previous runs output directory. metavirs install : Download reference files locally. metavirs cache : Cache software containers locally. metavirs is a comprehensive viral metagenomics pipeline to assemble, annotate, and classify microorganisms in enviromental samples. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from Illumina short-read sequencing technologies. As input, it accepts a set of FastQ files and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . Citation \u00b6 If you use this software, please cite it as below: BibTex @software{Kuhn_OpenOmics_metavirs_2022, author = {Kuhn, Skyler and Schaughency, Paul}, doi = {10.5281/zenodo.7120936}, month = {9}, title = {{OpenOmics/metavirs}}, url = {https://github.com/OpenOmics/metavirs/}, year = {2022} } APA Kuhn, S., & Schaughency, P. (2022). OpenOmics/metavirs [Computer software]. https://doi.org/10.5281/zenodo.7120936 For more citation style options, please visit the pipeline's Zenodo page . Contribute \u00b6 This site is a living document, created for and by members like you. metavirs is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository . References \u00b6 1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"About"},{"location":"#overview","text":"Welcome to our documentation! This guide is the main source of documentation for users that are getting started with the viral metagenomics pipeline . The ./metavirs pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: metavirs run : Run the metavirs pipeline with your input files. metavirs unlock : Unlocks a previous runs output directory. metavirs install : Download reference files locally. metavirs cache : Cache software containers locally. metavirs is a comprehensive viral metagenomics pipeline to assemble, annotate, and classify microorganisms in enviromental samples. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from Illumina short-read sequencing technologies. As input, it accepts a set of FastQ files and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github .","title":"Overview"},{"location":"#citation","text":"If you use this software, please cite it as below: BibTex @software{Kuhn_OpenOmics_metavirs_2022, author = {Kuhn, Skyler and Schaughency, Paul}, doi = {10.5281/zenodo.7120936}, month = {9}, title = {{OpenOmics/metavirs}}, url = {https://github.com/OpenOmics/metavirs/}, year = {2022} } APA Kuhn, S., & Schaughency, P. (2022). OpenOmics/metavirs [Computer software]. https://doi.org/10.5281/zenodo.7120936 For more citation style options, please visit the pipeline's Zenodo page .","title":"Citation"},{"location":"#contribute","text":"This site is a living document, created for and by members like you. metavirs is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"Contribute"},{"location":"#references","text":"1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"References"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"faq/questions/","text":"Frequently Asked Questions \u00b6 This page is still under construction. If you need immediate help, please open an issue on Github!","title":"General Questions"},{"location":"faq/questions/#frequently-asked-questions","text":"This page is still under construction. If you need immediate help, please open an issue on Github!","title":"Frequently Asked Questions"},{"location":"usage/cache/","text":"metavirs cache \u00b6 1. About \u00b6 The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, metavirs cache only has one required input . 2. Synopsis \u00b6 $ ./metavirs cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the metavirs run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the metavirs run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see metavirs run for more information. Example: --sif-cache /data/$USER/cache 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./metavirs cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./metavirs cache --sif-cache /data/ $USER /cache","title":"metavirs cache"},{"location":"usage/cache/#metavirs-cache","text":"","title":"metavirs cache"},{"location":"usage/cache/#1-about","text":"The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, metavirs cache only has one required input .","title":"1. About"},{"location":"usage/cache/#2-synopsis","text":"$ ./metavirs cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the metavirs run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/cache/#21-required-arguments","text":"--sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the metavirs run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see metavirs run for more information. Example: --sif-cache /data/$USER/cache","title":"2.1 Required Arguments"},{"location":"usage/cache/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run","title":"2.2 Options"},{"location":"usage/cache/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./metavirs cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./metavirs cache --sif-cache /data/ $USER /cache","title":"3. Example"},{"location":"usage/install/","text":"metavirs install \u00b6 1. About \u00b6 The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs install sub command in more detail. With minimal configuration, the install sub command enables you to download the pipeline's resource bundle locally. This is necessary when setting up the pipeline on a new target system or cluster. The pipeline uses a set of reference files to process the data. These reference files are required and need to be available on the local file system prior to execution. This command can be used to download any required reference files of the pipeline. Since most resource bundles are very large; we recommend using multiple threads for pulling reference files concurrently. The resource bundle can be very large so please ensure you have sufficent disk space prior to running this sub command. Please Note: The resource bundle is large and requires about 750 GB of available disk space. If you are running the pipeline on the Biowulf cluster, you do NOT need to download the pipeline's resource bundle. It is already accessible to all HPC users. Downloading the resource bundle is fast and easy! In its most basic form, metavirs install only has one required input . 2. Synopsis \u00b6 $ metavirs install [--help] [--dry-run] \\ [--force] [--threads] \\ --ref-path REF_PATH The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a output directory for the reference file download via the --ref-path argument. Once the download of the resource bundle has completed, a new child directory called metavirs will be created. This new directory will contain all of the pipeline's required reference files. The path to this new directory can be passed to the --resource-bundle option of the metavirs run subcomand. This allow users outside of Biowulf to run the pipeline. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --ref-path REF_PATH Path where the resource bundle will be downloaded. type: path Any resouces defined in the 'config/install.json' will be pulled onto the local filesystem. After the files have been downloaded, a new directory with the name metavirs will be created. It contains all the required reference files of the pipeline. The path to this new directory can be passed to the run sub command's --resource-bundle option. Please see the run sub command for more information. Example: --ref-path /data/$USER/refs 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Displays what remote resources would be pulled. Does not execute anything! Example: --dry-run --force Force downloads all files. type: boolean flag By default, any files that do not exist locally are pulled; however if a previous instance of an install did not exit gracefully, it may be necessary to forcefully re-download all the files. Example: --force --threads Number of threads to use for concurrent file downloads. type: int default: 2 Max number of threads to use for concurrent file downloads. Example: --threads 12 3. Example \u00b6 # Step 0.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 24gb --cpus-per-task = 12 --pty bash module purge module load singularity snakemake # Step 1.) Dry-run download of the resource bundle metavirs install --ref-path /data/ $USER /refs \\ --force \\ --dry-run \\ --threads 12 # Step 2.) Download the resource bundle, # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time to 2 days. metavirs install --ref-path /data/ $USER /refs \\ --force \\ --threads 12","title":"metavirs install"},{"location":"usage/install/#metavirs-install","text":"","title":"metavirs install"},{"location":"usage/install/#1-about","text":"The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs install sub command in more detail. With minimal configuration, the install sub command enables you to download the pipeline's resource bundle locally. This is necessary when setting up the pipeline on a new target system or cluster. The pipeline uses a set of reference files to process the data. These reference files are required and need to be available on the local file system prior to execution. This command can be used to download any required reference files of the pipeline. Since most resource bundles are very large; we recommend using multiple threads for pulling reference files concurrently. The resource bundle can be very large so please ensure you have sufficent disk space prior to running this sub command. Please Note: The resource bundle is large and requires about 750 GB of available disk space. If you are running the pipeline on the Biowulf cluster, you do NOT need to download the pipeline's resource bundle. It is already accessible to all HPC users. Downloading the resource bundle is fast and easy! In its most basic form, metavirs install only has one required input .","title":"1. About"},{"location":"usage/install/#2-synopsis","text":"$ metavirs install [--help] [--dry-run] \\ [--force] [--threads] \\ --ref-path REF_PATH The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a output directory for the reference file download via the --ref-path argument. Once the download of the resource bundle has completed, a new child directory called metavirs will be created. This new directory will contain all of the pipeline's required reference files. The path to this new directory can be passed to the --resource-bundle option of the metavirs run subcomand. This allow users outside of Biowulf to run the pipeline. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/install/#21-required-arguments","text":"--ref-path REF_PATH Path where the resource bundle will be downloaded. type: path Any resouces defined in the 'config/install.json' will be pulled onto the local filesystem. After the files have been downloaded, a new directory with the name metavirs will be created. It contains all the required reference files of the pipeline. The path to this new directory can be passed to the run sub command's --resource-bundle option. Please see the run sub command for more information. Example: --ref-path /data/$USER/refs","title":"2.1 Required Arguments"},{"location":"usage/install/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Displays what remote resources would be pulled. Does not execute anything! Example: --dry-run --force Force downloads all files. type: boolean flag By default, any files that do not exist locally are pulled; however if a previous instance of an install did not exit gracefully, it may be necessary to forcefully re-download all the files. Example: --force --threads Number of threads to use for concurrent file downloads. type: int default: 2 Max number of threads to use for concurrent file downloads. Example: --threads 12","title":"2.2 Options"},{"location":"usage/install/#3-example","text":"# Step 0.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 24gb --cpus-per-task = 12 --pty bash module purge module load singularity snakemake # Step 1.) Dry-run download of the resource bundle metavirs install --ref-path /data/ $USER /refs \\ --force \\ --dry-run \\ --threads 12 # Step 2.) Download the resource bundle, # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time to 2 days. metavirs install --ref-path /data/ $USER /refs \\ --force \\ --threads 12","title":"3. Example"},{"location":"usage/run/","text":"metavirs run \u00b6 1. About \u00b6 The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs run sub command in more detail. With minimal configuration, the run sub command enables you to start running metavirs pipeline. Setting up the metavirs pipeline is fast and easy! In its most basic form, metavirs run only has two required inputs . 2. Synopsis \u00b6 $ metavirs run [--help] [--aggregate] [--length-filter LENGTH_FILTER] \\ [--mode <slurm,local>] [--job-name JOB_NAME] \\ [--dry-run] [--silent] [--sif-cache SIF_CACHE] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--tmp-dir TMP_DIR] [--threads THREADS] \\ [--resource-bundle RESOURCE_BUNDLE] \\ --input INPUT [INPUT ...] \\ --output OUTPUT The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s). type: file(s) One or more FastQ files can be provided. The pipeline supports single-end and paired-end data. Single-end and paired-end FastQ files can be provided to this option simultaneously. Under the hood, the pipeline will run the correct set of rules and options for a given sample. When providing each input file, please seperate each file with a space. Globbing is supported! This makes selecting FastQ files easy. Note: Input FastQ files must be gzipp-ed. Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/metavirs_out 2.2 Analysis options \u00b6 Each of the following arguments are optional, and do not need to be provided. --aggregate Aggregate results into one report. type: boolean flag Aggregates contig annotation results into one mutli-sample, project-level interactive Krona report. By default, any resulting reports will be created at a per-sample level. Example: --aggregate --length-filter LENGTH_FILTER Filter contigs by total length (bp). type: integer default: 500 The contig length filter is used to remove any annotated contigs less than this threshold. Annotating small contigs can lead to lower-confidence classifications. This filter is only applied to metaspades contigs. Example: --length-filter 750 2.3 Orchestration options \u00b6 Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. slurm The slurm execution method will submit jobs to the SLURM workload manager . It is recommended running metavirs in this mode as execution will be significantly faster in a distributed environment. This is the default mode of execution. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. Example: --mode slurm --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:metavirs When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:metavirs\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The metavirs cache subcommand can be used to create a local SIF cache. Please see metavirs cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running metavirs with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --resource-bundle RESOURCE_BUNDLE Path to a resource bundle downloaded with the install sub command. type: path The resource bundle contains the set of required reference files for processing any data. The path provided to this option will be the path to the metavirs directory that was created when running the install sub command. Please see the install sub command for more information about downloading the pipeline's resource bundle. Example: --resource-bundle /data/$USER/refs/metavirs --threads THREADS Max number of threads for each process. type: integer default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /tmp/ Path on the file system for writing temporary output files. By default, the temporary directory is set to '/tmp/' for increased compatibility; however, if you are running the pipeline on a target system with a dedicated scratch space, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/ 2.4 Miscellaneous options \u00b6 Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help 3. Examples \u00b6 3.1 Biowulf \u00b6 # Step 0.) Grab an interactive node, # do not run on head node if you are # on a cluster, like Biowulf/BigSky! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash # Add any missing dependencies to $PATH module purge module load singularity module load snakemake # Step 1A.) Dry-run the pipeline, # this will display what steps will # be run or what steps still remain. ./metavirs run --input .tests/*.gz \\ --output /data/ $USER /metavirs_out \\ --mode slurm \\ --aggregate \\ --length-filter 500 \\ --dry-run # Step 1B.) Run the viral metagenomics # pipeline. The slurm mode will submit # jobs to the cluster. We recommended # running metavirs in this mode, if # you have access to a cluster like # Biowulf or BigSky. ./metavirs run --input .tests/*.gz \\ --output /data/ $USER /metavirs_out \\ --mode slurm \\ --aggregate \\ --length-filter 500 3.2 BigSky \u00b6 # Step 0.) Grab an interactive node, # do not run on head node if you are # on a cluster, like Biowulf/BigSky! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash # Add any missing dependencies to $PATH, # adds singularity, snakemake, and metavirs module purge source /gs1/RTS/OpenOmics/bin/dependencies.sh # Step 1A.) Dry-run the pipeline, # this will display what steps will # be run or what steps still remain. metavirs run --input .tests/*.gz \\ --output /gs1/Scratch/ $USER /metavirs_out \\ --mode slurm \\ --sif-cache /gs1/RTS/OpenOmics/SIFs/ \\ --resource-bundle /gs1/RTS/OpenOmics/references/metavirs/ \\ --aggregate \\ --length-filter 500 \\ --dry-run # Step 1B.) Run the viral metagenomics # pipeline. The slurm mode will submit # jobs to the cluster. We recommended # running metavirs in this mode, if # you have access to a cluster like # Biowulf or BigSky. metavirs run --input .tests/*.gz \\ --output /gs1/Scratch/ $USER /metavirs_out \\ --mode slurm \\ --sif-cache /gs1/RTS/OpenOmics/SIFs/ \\ --resource-bundle /gs1/RTS/OpenOmics/references/metavirs/ \\ --aggregate \\ --length-filter 500","title":"metavirs run"},{"location":"usage/run/#metavirs-run","text":"","title":"metavirs run"},{"location":"usage/run/#1-about","text":"The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs run sub command in more detail. With minimal configuration, the run sub command enables you to start running metavirs pipeline. Setting up the metavirs pipeline is fast and easy! In its most basic form, metavirs run only has two required inputs .","title":"1. About"},{"location":"usage/run/#2-synopsis","text":"$ metavirs run [--help] [--aggregate] [--length-filter LENGTH_FILTER] \\ [--mode <slurm,local>] [--job-name JOB_NAME] \\ [--dry-run] [--silent] [--sif-cache SIF_CACHE] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--tmp-dir TMP_DIR] [--threads THREADS] \\ [--resource-bundle RESOURCE_BUNDLE] \\ --input INPUT [INPUT ...] \\ --output OUTPUT The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/run/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s). type: file(s) One or more FastQ files can be provided. The pipeline supports single-end and paired-end data. Single-end and paired-end FastQ files can be provided to this option simultaneously. Under the hood, the pipeline will run the correct set of rules and options for a given sample. When providing each input file, please seperate each file with a space. Globbing is supported! This makes selecting FastQ files easy. Note: Input FastQ files must be gzipp-ed. Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/metavirs_out","title":"2.1 Required Arguments"},{"location":"usage/run/#22-analysis-options","text":"Each of the following arguments are optional, and do not need to be provided. --aggregate Aggregate results into one report. type: boolean flag Aggregates contig annotation results into one mutli-sample, project-level interactive Krona report. By default, any resulting reports will be created at a per-sample level. Example: --aggregate --length-filter LENGTH_FILTER Filter contigs by total length (bp). type: integer default: 500 The contig length filter is used to remove any annotated contigs less than this threshold. Annotating small contigs can lead to lower-confidence classifications. This filter is only applied to metaspades contigs. Example: --length-filter 750","title":"2.2 Analysis options"},{"location":"usage/run/#23-orchestration-options","text":"Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. slurm The slurm execution method will submit jobs to the SLURM workload manager . It is recommended running metavirs in this mode as execution will be significantly faster in a distributed environment. This is the default mode of execution. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. Example: --mode slurm --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:metavirs When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:metavirs\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The metavirs cache subcommand can be used to create a local SIF cache. Please see metavirs cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running metavirs with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --resource-bundle RESOURCE_BUNDLE Path to a resource bundle downloaded with the install sub command. type: path The resource bundle contains the set of required reference files for processing any data. The path provided to this option will be the path to the metavirs directory that was created when running the install sub command. Please see the install sub command for more information about downloading the pipeline's resource bundle. Example: --resource-bundle /data/$USER/refs/metavirs --threads THREADS Max number of threads for each process. type: integer default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /tmp/ Path on the file system for writing temporary output files. By default, the temporary directory is set to '/tmp/' for increased compatibility; however, if you are running the pipeline on a target system with a dedicated scratch space, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/","title":"2.3 Orchestration options"},{"location":"usage/run/#24-miscellaneous-options","text":"Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help","title":"2.4 Miscellaneous options"},{"location":"usage/run/#3-examples","text":"","title":"3. Examples"},{"location":"usage/run/#31-biowulf","text":"# Step 0.) Grab an interactive node, # do not run on head node if you are # on a cluster, like Biowulf/BigSky! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash # Add any missing dependencies to $PATH module purge module load singularity module load snakemake # Step 1A.) Dry-run the pipeline, # this will display what steps will # be run or what steps still remain. ./metavirs run --input .tests/*.gz \\ --output /data/ $USER /metavirs_out \\ --mode slurm \\ --aggregate \\ --length-filter 500 \\ --dry-run # Step 1B.) Run the viral metagenomics # pipeline. The slurm mode will submit # jobs to the cluster. We recommended # running metavirs in this mode, if # you have access to a cluster like # Biowulf or BigSky. ./metavirs run --input .tests/*.gz \\ --output /data/ $USER /metavirs_out \\ --mode slurm \\ --aggregate \\ --length-filter 500","title":"3.1 Biowulf"},{"location":"usage/run/#32-bigsky","text":"# Step 0.) Grab an interactive node, # do not run on head node if you are # on a cluster, like Biowulf/BigSky! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash # Add any missing dependencies to $PATH, # adds singularity, snakemake, and metavirs module purge source /gs1/RTS/OpenOmics/bin/dependencies.sh # Step 1A.) Dry-run the pipeline, # this will display what steps will # be run or what steps still remain. metavirs run --input .tests/*.gz \\ --output /gs1/Scratch/ $USER /metavirs_out \\ --mode slurm \\ --sif-cache /gs1/RTS/OpenOmics/SIFs/ \\ --resource-bundle /gs1/RTS/OpenOmics/references/metavirs/ \\ --aggregate \\ --length-filter 500 \\ --dry-run # Step 1B.) Run the viral metagenomics # pipeline. The slurm mode will submit # jobs to the cluster. We recommended # running metavirs in this mode, if # you have access to a cluster like # Biowulf or BigSky. metavirs run --input .tests/*.gz \\ --output /gs1/Scratch/ $USER /metavirs_out \\ --mode slurm \\ --sif-cache /gs1/RTS/OpenOmics/SIFs/ \\ --resource-bundle /gs1/RTS/OpenOmics/references/metavirs/ \\ --aggregate \\ --length-filter 500","title":"3.2 BigSky"},{"location":"usage/unlock/","text":"metavirs unlock \u00b6 1. About \u00b6 The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking metavirs pipeline output directory is fast and easy! In its most basic form, metavirs unlock only has one required input . 2. Synopsis \u00b6 $ ./metavirs unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/metavirs_out 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory metavirs unlock --output /data/ $USER /metavirs_out","title":"metavirs unlock"},{"location":"usage/unlock/#metavirs-unlock","text":"","title":"metavirs unlock"},{"location":"usage/unlock/#1-about","text":"The metavirs executable is composed of several inter-related sub commands. Please see metavirs -h for all available options. This part of the documentation describes options and concepts for metavirs unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking metavirs pipeline output directory is fast and easy! In its most basic form, metavirs unlock only has one required input .","title":"1. About"},{"location":"usage/unlock/#2-synopsis","text":"$ ./metavirs unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/unlock/#21-required-arguments","text":"--output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/metavirs_out","title":"2.1 Required Arguments"},{"location":"usage/unlock/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.2 Options"},{"location":"usage/unlock/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory metavirs unlock --output /data/ $USER /metavirs_out","title":"3. Example"}]}